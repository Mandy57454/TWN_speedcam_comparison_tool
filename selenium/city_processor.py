import time
import os
import openpyxl
import re
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import StaleElementReferenceException
import pandas as pd

project_path = r"C:\Users\mandy.chang\PycharmProjects\TWN_speedcam_compare"


def wait_for_page_load(driver, timeout=10):
    start_time = time.time()
    while time.time() - start_time < timeout:
        # check page readyState
        ready_state = driver.execute_script("return document.readyState")
        if ready_state == "complete":
            return True
        else:
            time.sleep(0.5)
    return False


def wait_for_element(driver, by, value, timeout=10):
    """
    ç­‰å¾…elementå¯è¦‹ä¸¦return element

    :param driver: WebDriver å¯¦ä¾‹
    :param by: å®šä½æ–¹å¼ï¼ˆå¦‚ By.ID, By.XPATH, By.PARTIAL_LINK_TEXT ç­‰ï¼‰
    :param value: å®šä½å€¼
    :param timeout: è¶…æ™‚æ™‚é–“ï¼Œé»˜èª10ç§’
    :return: å®šä½åˆ°çš„ WebElement
    """
    try:
        element = WebDriverWait(driver, timeout).until(
            EC.visibility_of_element_located((by, value))
        )
        return element
    except Exception as e:
        print(f"ç„¡æ³•å®šä½element: {e}")
        return None


def process_taipei(driver, info):
    url = info['url']
    keywords_index_s = info['keywords_index_s']
    try:
        for word, index_s in keywords_index_s.items():
            driver.get(url)
            wait_for_page_load(driver)
            element = driver.find_element(By.PARTIAL_LINK_TEXT, word)
            element.click()
            wait_for_page_load(driver)
            
            # æ‰¾åˆ°æ‰€æœ‰ PDF é€£çµ
            pdf_links = driver.find_elements(By.PARTIAL_LINK_TEXT, 'pdf')
            print(f"æ‰¾åˆ° {len(pdf_links)} å€‹ PDF æª”æ¡ˆ")
            
            # ä¸‹è¼‰æ‰€æœ‰ PDF æª”æ¡ˆ
            for i, pdf_link in enumerate(pdf_links):
                try:
                    print(f"æ­£åœ¨ä¸‹è¼‰ç¬¬ {i+1} å€‹ PDF æª”æ¡ˆ...")
                    pdf_link.click()
                    time.sleep(2)  # ç­‰å¾…ä¸‹è¼‰é–‹å§‹
                except Exception as e:
                    print(f"ä¸‹è¼‰ç¬¬ {i+1} å€‹ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    continue
                    
    except Exception as e:
        print(f"è™•ç†è‡ºåŒ—å¸‚è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise



def process_TaoYuan(driver, info):
    url = info['url']
    keywords_index_s = info['keywords_index_s']
    try:
        for word, index_s in keywords_index_s.items():
            driver.get(url)
            wait_for_page_load(driver)
            element = driver.find_element(By.PARTIAL_LINK_TEXT, word)
            element.click()
            wait_for_page_load(driver)
            
            # æ‰¾åˆ°æ‰€æœ‰ PDF é€£çµ
            pdf_links = driver.find_elements(By.PARTIAL_LINK_TEXT, 'pdf')
            print(f"æ‰¾åˆ° {len(pdf_links)} å€‹ PDF æª”æ¡ˆ")
            
            # ä¸‹è¼‰æ‰€æœ‰ PDF æª”æ¡ˆ
            for i, pdf_link in enumerate(pdf_links):
                try:
                    print(f"æ­£åœ¨ä¸‹è¼‰ç¬¬ {i+1} å€‹ PDF æª”æ¡ˆ...")
                    pdf_link.click()
                    time.sleep(2)  # ç­‰å¾…ä¸‹è¼‰é–‹å§‹
                except Exception as e:
                    print(f"ä¸‹è¼‰ç¬¬ {i+1} å€‹ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    continue
                    
    except Exception as e:
        print(f"è™•ç†æ¡ƒåœ’å¸‚è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise


def process_KeeLung(driver, info):
    url = info['url']
    keywords_index_s = info['keywords_index_s']
    try:
        for word, index_s in keywords_index_s.items():
            driver.get(url)
            wait_for_page_load(driver)
            element = driver.find_element(By.PARTIAL_LINK_TEXT, word)
            element.click()
            pdf_links = driver.find_elements(By.CLASS_NAME, 'fileType')
            for index in index_s:
                pdf_links[int(index)].click()
            time.sleep(5)
    finally:
        driver.quit()


def process_HsinChu(driver, info):
    url = info['url']
    keywords = info['keywords']
    wait = WebDriverWait(driver, 10)
    try:
        for kw in keywords:
            # 1) æ¯æ¬¡éƒ½å…ˆé‡æ–°è½½å…¥åˆ—è¡¨é¡µ
            driver.get(url)
            wait_for_page_load(driver)

            # 2) æ ¹æ® service_unit_title æ–‡æœ¬å»å®šä½å¯¹åº”çš„ <div>ï¼Œå†æ‹¿åˆ°å®ƒå¤–å±‚çš„ <a>
            title_div = wait.until(EC.presence_of_element_located((
                By.XPATH,
                f"//div[contains(@class,'service_unit_title') and contains(normalize-space(.), '{kw}')]"
            )))
            link = title_div.find_element(By.XPATH, "ancestor::a[1]")

            # 3) éšè—é®æŒ¡å±‚ï¼ˆå¦‚æœéœ€è¦ï¼‰
            driver.execute_script("""
                      const m = document.querySelector('div.modal-165');
                      if(m){ m.style.display='none'; }
                    """)

            # 4) æ»šåŠ¨åˆ°ä¸­é—´ & JS click
            driver.execute_script(
                "arguments[0].scrollIntoView({block:'center'});", link
            )
            time.sleep(0.2)
            driver.execute_script("arguments[0].click();", link)

            # 5) å¦‚æœç‚¹å‡»åç•™åœ¨æ–°é¡µé¢ï¼Œç­‰å®ƒåŠ è½½å®Œå†å›åˆ°åˆ—è¡¨é¡µ
            time.sleep(1)

    finally:
        driver.quit()


def process_newtaipei(driver, info):
    url = info['url']
    fixed_keywords = info['fixed']
    mobile_text = info['mobile']
    workbook = openpyxl.Workbook()
    wait = WebDriverWait(driver, 10)

    for fixed, keyWord in fixed_keywords.items():
        driver.get(url)
        wait_for_page_load(driver)
        element = driver.find_element(By.PARTIAL_LINK_TEXT, fixed)
        element.click()
        # pdf_links = driver.find_elements(By.PARTIAL_LINK_TEXT, keyWord)
        pdf = wait.until(EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, keyWord)))
        time.sleep(0.3)
        pdf.click()

        # å›åˆ°åŸåˆ—è¡¨é¡µï¼Œå¦åˆ™ä¸‹ä¸€æ¬¡å¾ªç¯ä½ å°±ä¸€ç›´åœ¨ PDF é¡µé¢ä¸Šäº†
        driver.back()
        time.sleep(1)
####################################################################
    driver.get(url)
    wait_for_page_load(driver)

    element = wait.until(EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, mobile_text)))
    element.click()
    # æŠ“è¡¨æ ¼
    table = wait.until(EC.presence_of_element_located((
        By.XPATH,
        f"//table[caption[contains(normalize-space(.), '{mobile_text}')]]"
    )))
    rows = table.find_elements(By.TAG_NAME, "tr")

    sheet = workbook.create_sheet(title=mobile_text)
    for r, row in enumerate(rows, start=1):
        cells = row.find_elements(By.XPATH, ".//th|.//td")
        for c, cell in enumerate(cells, start=1):
            sheet.cell(row=r, column=c, value=cell.text)
    # ç§»é™¤é»˜è®¤åˆ›å»ºçš„ç©ºç™½ sheet
    if 'Sheet' in workbook.sheetnames:
        workbook.remove(workbook['Sheet'])
    # å­˜æ¡£
    filename = f"{mobile_text}.xlsx"
    save_path = os.path.join(project_path, "selenium", "city_data", "NewTaipei", filename)
    workbook.save(save_path)
    time.sleep(1)

    driver.quit()


def process_Science_Park(driver, info):
    url = info['url']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        element = driver.find_element(By.XPATH, "//img[@src='/static/images/file/pdf.png']")
        element.click()
        # input("Press Enter to close the browser...")
        time.sleep(2)
    finally:
        driver.quit()


def process_MiaoLi_YunLin(driver, info):
    url = info['url']
    keywords_index_s = info['keywords_index_s']
    try:
        for word, index_s in keywords_index_s.items():
            driver.get(url)
            wait_for_page_load(driver)

            found = False
            while not found:
                try:
                    element = driver.find_element(By.PARTIAL_LINK_TEXT, word)
                    element.click()
                    found = True
                except NoSuchElementException:
                    try:
                        nextpage = driver.find_element(By.PARTIAL_LINK_TEXT, 'ä¸‹ä¸€é ')
                        nextpage.click()
                        wait_for_page_load(driver)
                    except NoSuchElementException:
                        print("No more pages and element not found")
                        break

            if found:
                pdf_links = driver.find_elements(By.XPATH, "//img[@src='/Content/img/d06.gif']")
                for index in index_s:
                    try:
                        pdf_links[int(index)].click()
                        time.sleep(3)  # ç­‰å¾…ä¸‹è½½æˆ–é¡µé¢åŠ è½½
                    except IndexError:
                        print(f"Index {index} is out of range for PDF links")
    finally:
        driver.quit()


def process_TaiChung(driver, info):
    url = info['url']
    keywords_index_s = info['keywords_index_s']
    try:
        for word, index_s in keywords_index_s.items():
            driver.get(url)
            wait_for_page_load(driver)
            element = driver.find_element(By.PARTIAL_LINK_TEXT, word)
            element.click()
            pdf_links = driver.find_elements(By.XPATH, "//img[@alt='PDF']")
            for index in index_s:
                pdf_links[int(index)].click()
            time.sleep(2)
    finally:
        driver.quit()


def process_ChiaYi_YiLan(driver, info):
    url = info['url']
    keywords_keywords = info['keywords_keyWords']
    try:
        for keyword, keyWord in keywords_keywords.items():
            driver.get(url)
            wait_for_page_load(driver)
            
            # ç›´æ¥ä½¿ç”¨éƒ¨åˆ†åŒ¹é…å°‹æ‰¾é—œéµå­—é€£çµ
            element = None
            
            # æ–¹æ³•1: éƒ¨åˆ†åŒ¹é… - å°‡é—œéµå­—åˆ†è§£æˆè©çµ„
            keyword_parts = keyword.split()
            for part in keyword_parts:
                if len(part) > 2:  # åªè™•ç†é•·åº¦å¤§æ–¼2çš„è©çµ„
                    try:
                        element = driver.find_element(By.PARTIAL_LINK_TEXT, part)
                        print(f"âœ… æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„é€£çµ (é—œéµå­—: '{part}'): {keyword}")
                        break
                    except NoSuchElementException:
                        continue
            
            # æ–¹æ³•2: å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œæœå°‹æ‰€æœ‰é€£çµ
            if element is None:
                all_links = driver.find_elements(By.TAG_NAME, "a")
                for link in all_links:
                    try:
                        link_text = link.text
                        # æª¢æŸ¥é€£çµæ–‡å­—æ˜¯å¦åŒ…å«é—œéµå­—çš„ä»»ä½•éƒ¨åˆ†
                        if any(part in link_text for part in keyword_parts if len(part) > 2):
                            element = link
                            print(f"âœ… æ‰¾åˆ°åŒ…å«é—œéµå­—çš„é€£çµ: {link_text}")
                            break
                    except:
                        continue
            
            if element is None:
                print(f"âŒ ç„¡æ³•æ‰¾åˆ°åŒ…å«é—œéµå­— '{keyword}' çš„é€£çµ")
                continue
                
            element.click()
            wait_for_page_load(driver)
            
            # å°‹æ‰¾ PDF é€£çµ - ç›´æ¥ä½¿ç”¨éƒ¨åˆ†åŒ¹é…
            pdf_links = []
            
            # æ–¹æ³•1: éƒ¨åˆ†åŒ¹é… PDF é—œéµå­—
            keyWord_parts = keyWord.split()
            for part in keyWord_parts:
                if len(part) > 2:
                    pdf_links = driver.find_elements(By.PARTIAL_LINK_TEXT, part)
                    if pdf_links:
                        print(f"âœ… æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„ PDF é€£çµ (é—œéµå­—: '{part}')")
                        break
            
            # æ–¹æ³•2: å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œæœå°‹æ‰€æœ‰ PDF é€£çµ
            if not pdf_links:
                pdf_links = driver.find_elements(By.PARTIAL_LINK_TEXT, 'pdf')
                if not pdf_links:
                    all_links = driver.find_elements(By.TAG_NAME, "a")
                    pdf_links = [link for link in all_links if 'pdf' in link.text.lower()]
            
            print(f"ğŸ“„ æ‰¾åˆ° {len(pdf_links)} å€‹ PDF é€£çµ")
            
            for i, pdf_link in enumerate(pdf_links):
                try:
                    print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è¼‰ç¬¬ {i+1} å€‹ PDF æª”æ¡ˆ...")
                    pdf_link.click()
                    time.sleep(2)
                except Exception as e:
                    print(f"âŒ ä¸‹è¼‰ç¬¬ {i+1} å€‹ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    continue
                    
            time.sleep(3)
    except Exception as e:
        print(f"âŒ è™•ç†å®œè˜­ç¸£è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise


def process_TaiNan(driver, info):
    url = info['url']
    keywords = info['keywords']
    try:
        for keyword in keywords:
            driver.get(url)
            wait_for_page_load(driver)
            element = driver.find_element(By.PARTIAL_LINK_TEXT, keyword)
            element.click()
            time.sleep(2)
    finally:
        driver.quit()


def process_TaiTung(driver, info):
    url = info['url']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        element = WebDriverWait(driver, 15).until(
            EC.element_to_be_clickable((By.XPATH, "//a[contains(@href,'.pdf')]"))
        )
        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", element)
        time.sleep(0.3)
        element.click()
        time.sleep(0.5)
        if len(driver.window_handles) > 1:
            driver.switch_to.window(driver.window_handles[-1])
        time.sleep(40)
    finally:
        driver.quit()


def process_HuaLian_KinMen(driver, info):
    url = info['url']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        pdf_links = driver.find_elements(By.CLASS_NAME, 'pdf')
        # if len(pdf_links) >= 2:
        # é»æ“Šç¬¬äºŒå€‹å…ƒç´ ï¼ˆç´¢å¼•å¾ 0 é–‹å§‹ï¼Œæ‰€ä»¥ç¬¬äºŒå€‹å…ƒç´ æ˜¯ç´¢å¼• 1ï¼‰
        #    pdf_links[1].click()
        # else:
        pdf_links[0].click()
        time.sleep(2)
    finally:
        driver.quit()


def process_Kaohsiung(driver, info):
    url = info['url']
    keywords_keywords = info['keywords_keyWords']

    # æ–°å»ºä¸€ä¸ªå·¥ä½œç°¿ï¼Œç”¨äºåˆå¹¶æ‰€æœ‰è¡¨æ ¼
    workbook = openpyxl.Workbook()

    try:
        for keyword, keyWord in keywords_keywords.items():
            driver.get(url)
            wait_for_page_load(driver)
            # element = driver.find_element(By.PARTIAL_LINK_TEXT, keyword)
            element = driver.find_element(By.XPATH, f"//a[text()[contains(., '{keyword}')]]")
            element.click()
            # table = driver.find_element(By.XPATH, "//table[caption[contains(text(), keyWord)]]")
            # å®šä½è¡¨æ ¼
            table = driver.find_element(By.XPATH, f"//table[caption[contains(normalize-space(.), '{keyWord}')]]")

            # å¦‚æœè¡¨å¤´ä½¿ç”¨ <td> è€Œé <th>ï¼Œåˆ™ä»ç¬¬ä¸€è¡Œçš„ <td> ä¸­æå–
            all_rows = table.find_elements(By.TAG_NAME, 'tr')
            # å‡è®¾ç¬¬ä¸€æ¡ <tr> ä¸ºè¡¨å¤´è¡Œ
            header_row = all_rows[0]
            header_elements = header_row.find_elements(By.TAG_NAME, 'td')
            headers = [h.text.strip() for h in header_elements]
            n_cols = len(headers)

            # å†™å…¥æ–° sheet
            sheet = workbook.create_sheet(title=keyword)
            # å†™å…¥è¡¨å¤´
            for col_idx, header in enumerate(headers, start=1):
                sheet.cell(row=1, column=col_idx, value=header)

            # å‡†å¤‡ rowspan å¤„ç†
            span_counters = [0] * n_cols
            prev_data = [''] * n_cols
            sheet_row = 2

            # éå†æ•°æ®è¡Œï¼Œä»ç¬¬äºŒè¡Œå¼€å§‹
            for row in all_rows[1:]:
                cells = row.find_elements(By.TAG_NAME, 'td')
                if not cells:
                    continue
                row_data = [''] * n_cols
                td_idx = 0
                for col_idx in range(n_cols):
                    if span_counters[col_idx] > 0:
                        row_data[col_idx] = prev_data[col_idx]
                        span_counters[col_idx] -= 1
                    else:
                        cell = cells[td_idx]
                        text = cell.text.strip()
                        row_data[col_idx] = text
                        rs = cell.get_attribute('rowspan')
                        if rs and rs.isdigit() and int(rs) > 1:
                            span_counters[col_idx] = int(rs) - 1
                        td_idx += 1

                # è·³è¿‡å…¨ç©ºè¡Œ
                if all(not v for v in row_data):
                    continue

                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                # print(f"Keyword={keyword}, å†™å…¥è¡Œ{sheet_row}: {row_data}")

                # å†™å…¥ Excel å¹¶å°è¯•å°†å¯è½¬ä¸ºæ•´æ•°çš„å€¼è½¬æ¢ä¸º int
                for col_idx, val in enumerate(row_data, start=1):
                    # å¦‚æœå­—ç¬¦ä¸²å…¨ä¸ºæ•°å­—ï¼Œåˆ™è½¬ä¸º int
                    write_val = int(val) if val.isdigit() else val
                    sheet.cell(row=sheet_row, column=col_idx, value=write_val)

                prev_data = row_data
                sheet_row += 1

            time.sleep(2)

            # ç§»é™¤é»˜è®¤åˆ›å»ºçš„ç©ºç™½ sheet
            if 'Sheet' in workbook.sheetnames:
                workbook.remove(workbook['Sheet'])

            # ä¿å­˜åˆå¹¶æ–‡ä»¶
            save_dir = os.path.join(project_path, 'selenium', 'city_data', 'Kaohsiung')
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, 'é«˜é›„å¸‚.xlsx')
            workbook.save(save_path)
    finally:
        driver.quit()


def process_ChiaYi_sh(driver, info):
    url = info['url']
    keywords_keywords = info['keywords_keyWords']
    try:
        for keyword, keyWord in keywords_keywords.items():
            driver.get(url)
            wait_for_page_load(driver)
            element = driver.find_element(By.PARTIAL_LINK_TEXT, keyword)
            driver.execute_script("document.querySelector('div.tab').style.display = 'none';")
            element.click()
            wait_for_page_load(driver)
            wait = WebDriverWait(driver, 10)

            download_box = wait.until(EC.presence_of_element_located((By.ID, "download_box")))
            pdf_links = download_box.find_elements(By.CSS_SELECTOR, "a[title$='.pdf']")
            for idx, link in enumerate(pdf_links, start=1):
                print(idx, link.get_attribute("title"), link.get_attribute("href"))
                # å¦‚æœè¦ç‚¹å‡»ï¼Œå¯ä»¥ï¼š
                driver.execute_script("arguments[0].click();", link)
            time.sleep(10)

    finally:
        driver.quit()


def process_PengHu(driver, info):
    url = info['url']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        wait = WebDriverWait(driver, 10)
        all_data = []
        headers = None
        page = 1

        while True:
            # ç­‰å¾… table è¼‰å…¥
            table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'table.table0.table02')))

            # æ¯æ¬¡éƒ½é‡æ–°å– row
            rows = table.find_elements(By.CSS_SELECTOR, 'tbody tr')

            # æå–è¡¨é ­ï¼ˆåƒ…æå–ä¸€æ¬¡ï¼‰
            if headers is None:
                headers = [th.text for th in table.find_elements(By.CSS_SELECTOR, 'thead th')]

            # æå–è¡¨æ ¼æ•¸æ“š
            for row in rows:
                try:
                    cells = row.find_elements(By.CSS_SELECTOR, 'td')
                    row_vals = []
                    for cell in cells:
                        txt = cell.text.strip()
                        # å¦‚æœå…¨æ˜¯æ•°å­—ï¼Œè½¬æˆ intï¼›å¦åˆ™ä¿ç•™åŸæ–‡æœ¬
                        if txt.isdigit():
                            row_vals.append(int(txt))
                        else:
                            row_vals.append(txt)
                    all_data.append(row_vals)
                except StaleElementReferenceException:
                    print("Row stale, skip this row")
                    continue

            # é»æ“Šã€Œä¸‹ä¸€é ã€
            try:
                next_page = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, "ä¸‹ä¸€é ")))
                next_page.click()
                print(f"å·²é»æ“Š ä¸‹ä¸€é ï¼Œç¬¬{page}é ")
                page += 1

                # é»æ“Šå¾Œå»ºè­°ç­‰å¾…æ–°çš„ table reload
                wait.until(EC.staleness_of(table))  # ç­‰èˆŠ table æ¶ˆå¤±
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'table.table0.table02')))  # å†ç­‰æ–° table å‡ºç¾

            except TimeoutException:
                print("æ²’æœ‰ä¸‹ä¸€é ï¼ŒçµæŸçˆ¬å–")
                break

        # å‰µå»º DataFrame ä¸¦ä¿å­˜ç‚º Excel æ–‡ä»¶
        import pandas as pd
        filename = f"{'æ¾æ¹–ç¸£'}.xlsx"
        save_path = os.path.join(project_path, r"selenium\city_data\PengHu", filename)
        df = pd.DataFrame(all_data, columns=headers)
        df.to_excel(save_path, index=False)
        time.sleep(2)
    finally:
        driver.quit()


def process_HsinChu_web(driver, info):
    url = info['url']
    city = info['city']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        main_content = driver.find_element(By.CLASS_NAME, "main-content")
        tables = main_content.find_elements(By.TAG_NAME, "table")
        print(f"æŠ“åˆ° {len(tables)} å€‹ tableï¼")
        for idx, table in enumerate(tables):
            workbook = openpyxl.Workbook()
            for idx, table in enumerate(tables):
                if idx == 0:
                    sheet = workbook.active
                    sheet.title = f"table{idx + 1}"
                else:
                    sheet = workbook.create_sheet(title=f"table{idx + 1}")
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row_index, row in enumerate(rows, start=1):
                    cells = row.find_elements(By.TAG_NAME, "th")
                    if not cells:
                        cells = row.find_elements(By.TAG_NAME, "td")
                    for cell_index, cell in enumerate(cells, start=1):
                        text = cell.text.strip()
                        try:
                            value = int(text)
                        except ValueError:
                            value = text
                        sheet.cell(row=row_index, column=cell_index, value=value)
            filename = f"{'æ–°ç«¹å¸‚'}.xlsx"
            save_path = os.path.join(project_path, r"selenium\city_data", city)
            workbook.save(os.path.join(save_path, filename))
    finally:
        driver.quit()


def process_ChangHua(driver, info):
    url = info['url']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        # driver.find_element(By.CSS_SELECTOR, ".kf_dload-xls").click()
        for el in driver.find_elements(By.CSS_SELECTOR, ".kf_dload-xls"):
            el.click()
            time.sleep(1)
        time.sleep(2)
    finally:
        driver.quit()


def process_NanTou(driver, info):
    url = info['url']
    try:
        driver.get(url)
        wait_for_page_load(driver)
        wait = WebDriverWait(driver, 10)
        all_sheets = []  # å­˜æ”¾æ¯ä¸€é çš„ DataFrame
        sheet_names = []  # å­˜æ”¾æ¯ä¸€é çš„åç¨±


        while True:
            # 1. å–å¾—ç›®å‰é ç¢¼
            page_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.page")))
            page_text = page_div.text
            match = re.search(r"(\d+)/(\d+)", page_text)
            if match:
                current_page = int(match.group(1))
            else:
                print("ç„¡æ³•å–å¾—ç›®å‰é ç¢¼")
                break

            # 2. æŠ“ul-liå…§å®¹
            ul = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.list.text_le')))
            lis = ul.find_elements(By.TAG_NAME, "li")
            if not lis:
                print("æœªæ‰¾åˆ°ä»»ä½•è³‡æ–™è¡Œï¼ŒçµæŸ")
                break

            # 3. æ¯ä¸€é éƒ½é‡æ–°æŠ“ header
            spans = lis[0].find_elements(By.TAG_NAME, "span")
            headers = [span.text.strip() for span in spans]
            data_lis = lis[1:]

            # 4. æ¯ç­†è³‡æ–™
            page_rows = []
            for li in data_lis:
                spans = li.find_elements(By.TAG_NAME, "span")
                row = [span.text.strip() for span in spans]
                page_rows.append(row)

            # å­˜æˆDataFrame
            df = pd.DataFrame(page_rows, columns=headers)
            all_sheets.append(df)
            sheet_names.append(f"ç¬¬{current_page}é ")

            # 5. é»ä¸‹ä¸€é ï¼Œç„¶å¾Œæ¯”å°é ç¢¼
            try:
                next_btn = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, "ä¸‹ä¸€é ")))
                next_btn.click()
                print("å·²é»æ“Š ä¸‹ä¸€é ")
                time.sleep(1)
            except Exception:
                print("æ²’æœ‰ä¸‹ä¸€é æŒ‰éˆ•ï¼ŒçµæŸçˆ¬å–")
                break

            # 6. ç¿»é å¾Œå†æ¬¡è®€é ç¢¼ï¼Œæ¯”è¼ƒæœ‰æ²’æœ‰è®Š
            page_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.page")))
            page_text = page_div.text
            match = re.search(r"(\d+)/(\d+)", page_text)
            if match:
                new_page = int(match.group(1))
            else:
                print("ç¿»é å¾Œç„¡æ³•å–å¾—é ç¢¼")
                break

            if new_page == current_page:
                print("é ç¢¼æ²’è®Šï¼Œå·²ç¶“æ˜¯æœ€å¾Œä¸€é ï¼ŒçµæŸ")
                break

        # å­˜æª”ï¼ˆæ¯ä¸€é ä¸€å€‹sheetï¼‰
        filename = f"{'å—æŠ•ç¸£'}.xlsx"
        save_path = os.path.join(project_path, r"selenium\city_data\NanTou", filename)
        with pd.ExcelWriter(save_path) as writer:
            for df, name in zip(all_sheets, sheet_names):
                # Excel sheet name æœ€é•·31å­—
                df.to_excel(writer, sheet_name=name[:31], index=False)
        time.sleep(2)
    finally:
        driver.quit()
